{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "# Training datasets\n",
    "cgm_train = pd.read_csv('cgm_train.csv')\n",
    "image_train = pd.read_csv('img_train.csv')\n",
    "demo_viome_train = pd.read_csv('demo_viome_train.csv')\n",
    "label_train = pd.read_csv('label_train.csv')\n",
    "\n",
    "# Test datasets\n",
    "cgm_test = pd.read_csv('cgm_test.csv')\n",
    "image_test = pd.read_csv('img_test.csv')\n",
    "demo_viome_test = pd.read_csv('demo_viome_test.csv')\n",
    "label_test = pd.read_csv('label_test_breakfast_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process CGM Data (Time-Series Glucose Levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if CGM Data is an empty array\n",
    "def is_cgm_data_empty(row):\n",
    "    try:\n",
    "        cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "        return len(cgm_list) == 0\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# Function to filter out rows where CGM Data is empty\n",
    "cgm_train = cgm_train[~cgm_train.apply(is_cgm_data_empty, axis=1)]\n",
    "\n",
    "# Handle missing breakfast and lunch times\n",
    "cgm_train['Breakfast Time'] = pd.to_datetime(cgm_train['Breakfast Time'], errors='coerce')\n",
    "cgm_train['Lunch Time'] = pd.to_datetime(cgm_train['Lunch Time'], errors='coerce')\n",
    "\n",
    "# Extract CGM data as list of tuples, convert to list of time series values\n",
    "cgm_train['CGM Data'] = cgm_train['CGM Data'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Extract features from CGM data (flatten the time and glucose values)\n",
    "def extract_cgm_features(cgm_data):\n",
    "    times = [entry[0] for entry in cgm_data]\n",
    "    glucose_levels = [entry[1] for entry in cgm_data]\n",
    "    return times, glucose_levels\n",
    "\n",
    "cgm_train['CGM Times'], cgm_train['CGM Levels'] = zip(*cgm_train['CGM Data'].apply(extract_cgm_features))\n",
    "\n",
    "# Normalize glucose levels\n",
    "scaler = StandardScaler()\n",
    "cgm_train['CGM Levels'] = cgm_train['CGM Levels'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with empty CGM data have been removed. TODO: Put in function to reuse for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pad the sequences to a fixed length for GRU input\n",
    "max_sequence_length = 300  # Define a maximum length for the sequences\n",
    "\n",
    "# Pad sequences of CGM data\n",
    "X = pad_sequences(cgm_train['CGM Levels'], maxlen=max_sequence_length, padding='post', value=0, dtype='float32')\n",
    "\n",
    "# Mask labels: We will use NaN or a predefined mask value for missing times -- 1\n",
    "cgm_train['Breakfast Time Masked'] = cgm_train['Breakfast Time'].isna().astype(int)\n",
    "cgm_train['Lunch Time Masked'] = cgm_train['Lunch Time'].isna().astype(int)\n",
    "\n",
    "# Prepare the target variable: encode the time values for breakfast and lunch\n",
    "def encode_times(time_column):\n",
    "    return (time_column - pd.Timestamp('2019-09-18')) // pd.Timedelta('1s')\n",
    "\n",
    "cgm_train['Breakfast Time Encoded'] = encode_times(cgm_train['Breakfast Time'])\n",
    "cgm_train['Lunch Time Encoded'] = encode_times(cgm_train['Lunch Time'])\n",
    "\n",
    "# For the target, we want to predict encoded times where it's available\n",
    "y_breakfast = cgm_train['Breakfast Time Encoded'].values\n",
    "y_lunch = cgm_train['Lunch Time Encoded'].values\n",
    "\n",
    "cgm_train.to_csv('cleaned_data.csv')\n",
    "\n",
    "# Mask missing values in the target variables for masked prediction\n",
    "y_breakfast_masked = np.where(cgm_train['Breakfast Time Masked'] == 0, y_breakfast, 0.0)\n",
    "y_lunch_masked = np.where(cgm_train['Lunch Time Masked'] == 0, y_lunch, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('output.txt', y_breakfast_masked, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maila\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking_11 (\u001b[38;5;33mMasking\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_11 (\u001b[38;5;33mGRU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m50,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,625</span> (229.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,625\u001b[0m (229.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,625</span> (229.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,625\u001b[0m (229.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Masking\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "\n",
    "# Masking layer to ignore padding during training\n",
    "model.add(Masking(mask_value=0., input_shape=(max_sequence_length, 1)))\n",
    "\n",
    "# GRU layers\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for predicting breakfast and lunch times (regression problem)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - loss: 8756759174840320.0000 - val_loss: 8291516506177536.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - loss: 8895275628232704.0000 - val_loss: 8291516506177536.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - loss: 8947575981867008.0000 - val_loss: 8291514358693888.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 9159788705349632.0000 - val_loss: 8291514358693888.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 9076891541569536.0000 - val_loss: 8291512211210240.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 8986005201747968.0000 - val_loss: 8291511137468416.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 9225260113068032.0000 - val_loss: 8291510063726592.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 9250500562124800.0000 - val_loss: 8291508453113856.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 9238514382143488.0000 - val_loss: 8291507379372032.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 9152754622660608.0000 - val_loss: 8291505231888384.0000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 8177926432358400.0000\n",
      "Validation Loss: 8291505231888384.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "X = np.expand_dims(X, axis=-1)  # Add a channel dimension for GRU input\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_breakfast_masked, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[64.14637 ],\n",
       "       [64.13842 ],\n",
       "       [64.1451  ],\n",
       "       [64.14146 ],\n",
       "       [64.148834],\n",
       "       [64.14213 ],\n",
       "       [64.129715],\n",
       "       [64.1469  ],\n",
       "       [64.14532 ],\n",
       "       [64.1467  ],\n",
       "       [64.15348 ],\n",
       "       [64.14712 ],\n",
       "       [64.139626],\n",
       "       [64.14457 ],\n",
       "       [64.130035],\n",
       "       [64.14807 ],\n",
       "       [64.145996],\n",
       "       [64.15392 ],\n",
       "       [64.14655 ],\n",
       "       [64.145294],\n",
       "       [64.151794],\n",
       "       [64.15048 ],\n",
       "       [64.14708 ],\n",
       "       [64.131996],\n",
       "       [64.139366],\n",
       "       [64.13807 ],\n",
       "       [64.13343 ],\n",
       "       [64.14701 ],\n",
       "       [64.043335],\n",
       "       [64.1446  ],\n",
       "       [64.14582 ],\n",
       "       [64.155266],\n",
       "       [64.15049 ],\n",
       "       [64.1486  ],\n",
       "       [64.133804],\n",
       "       [64.145485],\n",
       "       [64.145424],\n",
       "       [64.14951 ],\n",
       "       [64.13933 ],\n",
       "       [64.14805 ],\n",
       "       [64.13708 ],\n",
       "       [64.14672 ],\n",
       "       [64.1444  ],\n",
       "       [64.13642 ],\n",
       "       [64.14362 ],\n",
       "       [64.141396],\n",
       "       [64.12594 ],\n",
       "       [64.15031 ],\n",
       "       [64.14416 ],\n",
       "       [64.14529 ],\n",
       "       [64.14824 ],\n",
       "       [64.15009 ],\n",
       "       [64.14926 ],\n",
       "       [64.14964 ],\n",
       "       [64.130035],\n",
       "       [64.147514],\n",
       "       [64.14648 ],\n",
       "       [64.14631 ],\n",
       "       [64.12736 ],\n",
       "       [64.14514 ],\n",
       "       [64.12739 ],\n",
       "       [64.13199 ],\n",
       "       [64.14341 ],\n",
       "       [64.146164]], dtype=float32)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict missing breakfast times (masked values)\n",
    "predicted_breakfast_times = model.predict(X_val)\n",
    "\n",
    "predicted_breakfast_times\n",
    "\n",
    "# Convert the predicted time in seconds back to datetime format\n",
    "# predicted_breakfast_times = pd.to_datetime(predicted_breakfast_times, unit='s', origin='1970-01-01')\n",
    "\n",
    "# # You can use a similar approach for lunch time prediction\n",
    "# predicted_lunch_times = model.predict(X_val)\n",
    "# predicted_lunch_times = pd.to_datetime(predicted_lunch_times, unit='s', origin='1970-01-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load dataset with proper delimiter (ensure '\\t' for tab-separated values)\n",
    "# file_path = 'demo_viome_train.csv'\n",
    "# data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# # Recheck column parsing\n",
    "# if len(data.columns) == 1:\n",
    "#     # If all data is in a single column, try splitting with a comma\n",
    "#     data = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# # Verify column names\n",
    "# print(\"Columns in dataset after re-parsing:\", data.columns)\n",
    "\n",
    "# # Split the `Viome` column into individual features\n",
    "# viome_split = data['Viome'].str.split(',', expand=True).astype(float)\n",
    "# viome_split.columns = [f\"Viome_{i}\" for i in range(viome_split.shape[1])]\n",
    "\n",
    "# # Drop the original Viome column and merge new features\n",
    "# data = pd.concat([data.drop(columns=['Viome']), viome_split], axis=1)\n",
    "\n",
    "# # Impute missing values for numeric columns\n",
    "# numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "# # Normalize numeric data\n",
    "# scaler = MinMaxScaler()\n",
    "# data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "\n",
    "# # Encode categorical columns\n",
    "# categorical_cols = ['Gender', 'Race', 'Diabetes Status']\n",
    "# encoder = OneHotEncoder(sparse_output=False, drop='first')  # Use sparse_output instead of sparse\n",
    "# encoded_cats = pd.DataFrame(\n",
    "#     encoder.fit_transform(data[categorical_cols]),\n",
    "#     columns=encoder.get_feature_names_out(categorical_cols)\n",
    "# )\n",
    "\n",
    "# # Drop original categorical columns and merge encoded ones\n",
    "# data = pd.concat([data.drop(columns=categorical_cols), encoded_cats], axis=1)\n",
    "\n",
    "# # Final processed data\n",
    "# print(\"Processed Data Shape:\", data.shape)\n",
    "# print(\"Processed Data Preview:\")\n",
    "# print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv(\"img_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Placeholder for missing images (a blank black image)\n",
    "# def create_placeholder_image(size=(64, 64, 3)):\n",
    "#     return np.zeros(size, dtype=np.float32)  # Normalized [0, 1] range\n",
    "\n",
    "# # Function to preprocess image data\n",
    "# def preprocess_image(img_data, size=(64, 64)):\n",
    "#     try:\n",
    "#         img_array = np.array(img_data, dtype=np.uint8)  # Ensure valid data type\n",
    "\n",
    "#         # Check for empty image\n",
    "#         if img_array.size == 0 or img_array.ndim != 3 or img_array.shape[2] != 3:\n",
    "#             raise ValueError(f\"Invalid or empty image dimensions: {img_array.shape}\")\n",
    "\n",
    "#         img_resized = np.array(Image.fromarray(img_array).resize(size))  # Resize\n",
    "#         img_normalized = img_resized / 255.0  # Normalize pixel values to [0, 1]\n",
    "#         return img_normalized\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error preprocessing image: {e}\")\n",
    "#         return create_placeholder_image(size)\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# def preprocess_dataset(data):\n",
    "#     # Define placeholder image\n",
    "#     placeholder_image = create_placeholder_image()\n",
    "\n",
    "#     # Create missingness indicators\n",
    "#     data['Breakfast_Missing'] = data['Image Before Breakfast'].isnull().astype(int)\n",
    "#     data['Lunch_Missing'] = data['Image Before Lunch'].isnull().astype(int)\n",
    "\n",
    "#     # Iterate over rows to preprocess images\n",
    "#     breakfast_images = []\n",
    "#     lunch_images = []\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         # Handle missing breakfast images\n",
    "#         if pd.isnull(row['Image Before Breakfast']) or row['Image Before Breakfast'] == '[]':  # Check for empty list or NaN\n",
    "#             breakfast_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Breakfast'])  # Convert string to list\n",
    "#                 breakfast_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, breakfast: {e}\")\n",
    "#                 breakfast_images.append(placeholder_image)\n",
    "\n",
    "#         # Handle missing lunch images\n",
    "#         if pd.isnull(row['Image Before Lunch']) or row['Image Before Lunch'] == '[]':  # Check for empty list or NaN\n",
    "#             lunch_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Lunch'])  # Convert string to list\n",
    "#                 lunch_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, lunch: {e}\")\n",
    "#                 lunch_images.append(placeholder_image)\n",
    "\n",
    "#     # Add preprocessed images back to the dataset\n",
    "#     data['Processed_Breakfast_Images'] = breakfast_images\n",
    "#     data['Processed_Lunch_Images'] = lunch_images\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # Apply preprocessing\n",
    "# processed_data = preprocess_dataset(data)\n",
    "\n",
    "# # Save the processed dataset if needed\n",
    "# # processed_data.to_pickle(\"processed_img_train.pkl\")  # Save in pickle format for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# label_data = pd.read_csv(\"label_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Step 1: Extract Output Labels\n",
    "# output_labels = label_data[[\"Breakfast Calories\", \"Lunch Calories\"]]\n",
    "\n",
    "# # Step 2: Handle Missing Values in Labels\n",
    "# # Replace missing values (if any) with the median\n",
    "# output_labels = output_labels.fillna(output_labels.median())\n",
    "\n",
    "# print(output_labels)\n",
    "\n",
    "# # Step 3: Save the Extracted Labels\n",
    "# # output_labels.to_csv(\"output_labels.csv\", index=False)\n",
    "\n",
    "# print(\"Output Labels Extracted and Saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
