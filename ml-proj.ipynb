{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch.nn as nn\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "# Training datasets\n",
    "cgm_train = pd.read_csv('cgm_train.csv')\n",
    "image_train = pd.read_csv('img_train.csv')\n",
    "demo_viome_train = pd.read_csv('demo_viome_train.csv')\n",
    "label_train = pd.read_csv('label_train.csv')\n",
    "\n",
    "# Test datasets\n",
    "cgm_test = pd.read_csv('cgm_test.csv')\n",
    "image_test = pd.read_csv('img_test.csv')\n",
    "demo_viome_test = pd.read_csv('demo_viome_test.csv')\n",
    "label_test = pd.read_csv('label_test_breakfast_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process CGM Data (Time-Series Glucose Levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating missing values with mean\n",
    "\n",
    "# Function to check if CGM Data is an empty array\n",
    "def is_cgm_data_empty(row):\n",
    "    try:\n",
    "        cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "        return len(cgm_list) == 0\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# Function to filter out rows where CGM Data is empty\n",
    "cgm_train = cgm_train[~cgm_train.apply(is_cgm_data_empty, axis=1)]\n",
    "\n",
    "# Convert Breakfast and Lunch times to datetime\n",
    "# Convert Breakfast and Lunch times to datetime\n",
    "cgm_train['Breakfast Time'] = pd.to_datetime(cgm_train['Breakfast Time'], errors='coerce')\n",
    "cgm_train['Lunch Time'] = pd.to_datetime(cgm_train['Lunch Time'], errors='coerce')\n",
    "\n",
    "# Extract the date part and time part separately\n",
    "cgm_train['Breakfast Date'] = cgm_train['Breakfast Time'].dt.date\n",
    "cgm_train['Lunch Date'] = cgm_train['Lunch Time'].dt.date\n",
    "\n",
    "# Function to calculate mean time while preserving date\n",
    "def mean_time(times):\n",
    "    # Convert the times to minutes since midnight\n",
    "    total_seconds = sum([t.hour * 3600 + t.minute * 60 + t.second for t in times if pd.notna(t)])\n",
    "    mean_seconds = total_seconds // len([t for t in times if pd.notna(t)])\n",
    "    \n",
    "    # Convert the mean seconds back to HH:MM:SS\n",
    "    return pd.to_datetime(mean_seconds, unit='s').time()\n",
    "\n",
    "# Calculate the mean breakfast and lunch times per Subject ID\n",
    "mean_times = cgm_train.groupby('Subject ID')[['Breakfast Time', 'Lunch Time']].apply(\n",
    "    lambda group: pd.Series({\n",
    "        'Breakfast Time': mean_time(group['Breakfast Time']),\n",
    "        'Lunch Time': mean_time(group['Lunch Time'])\n",
    "    })\n",
    ")\n",
    "\n",
    "# Merge the date back with the mean times\n",
    "cgm_train['Breakfast Time'] = cgm_train['Breakfast Date'].astype(str) + ' ' + cgm_train['Breakfast Time'].fillna(cgm_train['Subject ID'].map(mean_times['Breakfast Time'])).astype(str)\n",
    "cgm_train['Lunch Time'] = cgm_train['Lunch Date'].astype(str) + ' ' + cgm_train['Lunch Time'].fillna(cgm_train['Subject ID'].map(mean_times['Lunch Time'])).astype(str)\n",
    "\n",
    "# Convert back to datetime\n",
    "cgm_train['Breakfast Time'] = pd.to_datetime(cgm_train['Breakfast Time'])\n",
    "cgm_train['Lunch Time'] = pd.to_datetime(cgm_train['Lunch Time'])\n",
    "\n",
    "# Drop the helper date columns\n",
    "cgm_train.drop(columns=['Breakfast Date', 'Lunch Date'], inplace=True)\n",
    "\n",
    "\n",
    "cgm_train.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to check if CGM Data is an empty array\n",
    "# def is_cgm_data_empty(row):\n",
    "#     try:\n",
    "#         cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "#         return len(cgm_list) == 0\n",
    "#     except:\n",
    "#         return True\n",
    "\n",
    "# # Function to filter out rows where CGM Data is empty\n",
    "# cgm_train = cgm_train[~cgm_train.apply(is_cgm_data_empty, axis=1)]\n",
    "\n",
    "# # Handle missing breakfast and lunch times\n",
    "# cgm_train['Breakfast Time'] = pd.to_datetime(cgm_train['Breakfast Time'], errors='coerce')\n",
    "# cgm_train['Lunch Time'] = pd.to_datetime(cgm_train['Lunch Time'], errors='coerce')\n",
    "\n",
    "# # Extract CGM data as list of tuples, convert to list of time series values\n",
    "# cgm_train['CGM Data'] = cgm_train['CGM Data'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# # Extract features from CGM data (flatten the time and glucose values)\n",
    "# def extract_cgm_features(cgm_data):\n",
    "#     times = [entry[0] for entry in cgm_data]\n",
    "#     glucose_levels = [entry[1] for entry in cgm_data]\n",
    "#     return times, glucose_levels\n",
    "\n",
    "# cgm_train['CGM Times'], cgm_train['CGM Levels'] = zip(*cgm_train['CGM Data'].apply(extract_cgm_features))\n",
    "\n",
    "# # Normalize glucose levels\n",
    "# scaler = StandardScaler()\n",
    "# cgm_train['CGM Levels'] = cgm_train['CGM Levels'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with empty CGM data have been removed. TODO: Put in function to reuse for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need to pad the sequences to a fixed length for GRU input\n",
    "# max_sequence_length = 300  # Define a maximum length for the sequences\n",
    "# cgm_train['Padded CGM Levels'] = pad_sequences(cgm_train['CGM Levels'], maxlen=max_sequence_length, padding='post', value=0, dtype='float32').tolist()\n",
    "\n",
    "# # Mask labels: We will use NaN or a predefined mask value for missing times -- 1\n",
    "# cgm_train['Breakfast Time Masked'] = cgm_train['Breakfast Time'].isna().astype(int)\n",
    "# cgm_train['Lunch Time Masked'] = cgm_train['Lunch Time'].isna().astype(int)\n",
    "\n",
    "# # Prepare the target variable: encode the time values for breakfast and lunch\n",
    "# def encode_times(time_column):\n",
    "#     return (time_column - pd.Timestamp('2021-09-18')) // pd.Timedelta('1s')\n",
    "\n",
    "# # Filter rows where both Breakfast and Lunch times are missing (i.e., both masks are 0)\n",
    "# filtered_cgm_train = cgm_train[(cgm_train['Breakfast Time Masked'] == 0) & (cgm_train['Lunch Time Masked'] == 0)].copy()\n",
    "\n",
    "# # Encode breakfast and lunch times only for rows where both are missing\n",
    "# filtered_cgm_train['Breakfast Time Encoded'] = encode_times(filtered_cgm_train['Breakfast Time'])\n",
    "# filtered_cgm_train['Lunch Time Encoded'] = encode_times(filtered_cgm_train['Lunch Time'])\n",
    "\n",
    "# time_scaler = MinMaxScaler()\n",
    "\n",
    "# # Reshape and scale both 'Breakfast Time Encoded' and 'Lunch Time Encoded'\n",
    "# filtered_cgm_train[['Breakfast Time Encoded', 'Lunch Time Encoded']] = time_scaler.fit_transform(\n",
    "#     filtered_cgm_train[['Breakfast Time Encoded', 'Lunch Time Encoded']]\n",
    "# )\n",
    "\n",
    "# X_train = np.array(filtered_cgm_train['Padded CGM Levels'].tolist())\n",
    "# y_train = filtered_cgm_train[['Breakfast Time Encoded', 'Lunch Time Encoded']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape X_train to (samples, time steps, features)\n",
    "# X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # (samples, time steps, features)\n",
    "\n",
    "# # Define the GRU model\n",
    "# def create_gru_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=input_shape),\n",
    "#         GRU(32, activation='relu'),\n",
    "#         Dense(2)  # Output two values: breakfast and lunch times\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001, clipvalue=1.0), loss='mse')\n",
    "#     return model\n",
    "\n",
    "# # Create and compile the model\n",
    "# model = create_gru_model((X_train_reshaped.shape[1], 1))\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_meal_times(model, X):\n",
    "#     X = np.array(X.tolist()) if isinstance(X, pd.Series) else np.array(X)\n",
    "#     X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "#     predictions = model.predict(X_reshaped)\n",
    "#     reference_date = pd.Timestamp('2021-09-18')\n",
    "\n",
    "#     # Decode predictions back to original scale using inverse transformation\n",
    "#     decoded_times = time_scaler.inverse_transform(predictions)\n",
    "\n",
    "#     # Add the decoded seconds back to the reference date\n",
    "#     decoded_breakfast_timestamps = reference_date + pd.to_timedelta(decoded_times[:, 0], unit='s')\n",
    "#     decoded_lunch_timestamps = reference_date + pd.to_timedelta(decoded_times[:, 1], unit='s')\n",
    "\n",
    "#     decoded_predictions = pd.DataFrame({\n",
    "#         'Predicted Breakfast Time': decoded_breakfast_timestamps,\n",
    "#         'Predicted Lunch Time': decoded_lunch_timestamps\n",
    "#     })\n",
    "\n",
    "#     return decoded_predictions\n",
    "\n",
    "# # Extract rows with missing breakfast or lunch times\n",
    "# missing_data = cgm_train[cgm_train['Breakfast Time'].isna() | cgm_train['Lunch Time'].isna()]\n",
    "\n",
    "# # Ensure that `Padded CGM Levels` is included in `missing_data`\n",
    "# predict_missing = missing_data['Padded CGM Levels']\n",
    "# missing_data_copy = missing_data.copy()\n",
    "\n",
    "# # Make predictions for missing breakfast and lunch times\n",
    "# predicted_times = predict_meal_times(model, predict_missing)\n",
    "\n",
    "# # Reset indices for both DataFrames to align by row order\n",
    "# missing_data_copy = missing_data_copy.reset_index(drop=True)\n",
    "# predicted_times = predicted_times.reset_index(drop=True)\n",
    "\n",
    "# # Add the 'Predicted Breakfast Time' column\n",
    "# missing_data_copy['Predicted Breakfast Time'] = predicted_times['Predicted Breakfast Time']\n",
    "# missing_data_copy['Predicted Lunch Time'] = predicted_times['Predicted Lunch Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('output.txt', y_breakfast_masked, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUPredictor(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers):\n",
    "#         super(GRUPredictor, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.gru(x, h0)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "# model = GRUPredictor(input_size=1, hidden_size=64, num_layers=2)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_tensor = torch.FloatTensor(X.reshape(-1, max_sequence_length, 1))\n",
    "# y_tensor = torch.FloatTensor(y_breakfast_masked_train)\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     outputs = model(X_tensor)\n",
    "#     loss = criterion(outputs, y_tensor)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f'Epoch: {epoch} Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense, Masking\n",
    "\n",
    "# # Build the GRU model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Masking layer to ignore padding during training\n",
    "# model.add(Masking(mask_value=0., input_shape=(max_sequence_length, 1)))\n",
    "\n",
    "# # GRU layers\n",
    "# model.add(GRU(128, return_sequences=False))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# # Output layer for predicting breakfast and lunch times (regression problem)\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Model summary\n",
    "# model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the data for training\n",
    "# X = np.expand_dims(X, axis=-1)  # Add a channel dimension for GRU input\n",
    "\n",
    "# # Split into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y_breakfast_masked, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_val, y_val)\n",
    "# print(f'Validation Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict missing breakfast times (masked values)\n",
    "# predicted_breakfast_times = model.predict(X_val)\n",
    "\n",
    "# predicted_breakfast_times\n",
    "\n",
    "# Convert the predicted time in seconds back to datetime format\n",
    "# predicted_breakfast_times = pd.to_datetime(predicted_breakfast_times, unit='s', origin='1970-01-01')\n",
    "\n",
    "# # You can use a similar approach for lunch time prediction\n",
    "# predicted_lunch_times = model.predict(X_val)\n",
    "# predicted_lunch_times = pd.to_datetime(predicted_lunch_times, unit='s', origin='1970-01-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load dataset with proper delimiter (ensure '\\t' for tab-separated values)\n",
    "# file_path = 'demo_viome_train.csv'\n",
    "# data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# # Recheck column parsing\n",
    "# if len(data.columns) == 1:\n",
    "#     # If all data is in a single column, try splitting with a comma\n",
    "#     data = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# # Verify column names\n",
    "# print(\"Columns in dataset after re-parsing:\", data.columns)\n",
    "\n",
    "# # Split the `Viome` column into individual features\n",
    "# viome_split = data['Viome'].str.split(',', expand=True).astype(float)\n",
    "# viome_split.columns = [f\"Viome_{i}\" for i in range(viome_split.shape[1])]\n",
    "\n",
    "# # Drop the original Viome column and merge new features\n",
    "# data = pd.concat([data.drop(columns=['Viome']), viome_split], axis=1)\n",
    "\n",
    "# # Impute missing values for numeric columns\n",
    "# numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "# # Normalize numeric data\n",
    "# scaler = MinMaxScaler()\n",
    "# data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "\n",
    "# # Encode categorical columns\n",
    "# categorical_cols = ['Gender', 'Race', 'Diabetes Status']\n",
    "# encoder = OneHotEncoder(sparse_output=False, drop='first')  # Use sparse_output instead of sparse\n",
    "# encoded_cats = pd.DataFrame(\n",
    "#     encoder.fit_transform(data[categorical_cols]),\n",
    "#     columns=encoder.get_feature_names_out(categorical_cols)\n",
    "# )\n",
    "\n",
    "# # Drop original categorical columns and merge encoded ones\n",
    "# data = pd.concat([data.drop(columns=categorical_cols), encoded_cats], axis=1)\n",
    "\n",
    "# # Final processed data\n",
    "# print(\"Processed Data Shape:\", data.shape)\n",
    "# print(\"Processed Data Preview:\")\n",
    "# print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv(\"img_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Placeholder for missing images (a blank black image)\n",
    "# def create_placeholder_image(size=(64, 64, 3)):\n",
    "#     return np.zeros(size, dtype=np.float32)  # Normalized [0, 1] range\n",
    "\n",
    "# # Function to preprocess image data\n",
    "# def preprocess_image(img_data, size=(64, 64)):\n",
    "#     try:\n",
    "#         img_array = np.array(img_data, dtype=np.uint8)  # Ensure valid data type\n",
    "\n",
    "#         # Check for empty image\n",
    "#         if img_array.size == 0 or img_array.ndim != 3 or img_array.shape[2] != 3:\n",
    "#             raise ValueError(f\"Invalid or empty image dimensions: {img_array.shape}\")\n",
    "\n",
    "#         img_resized = np.array(Image.fromarray(img_array).resize(size))  # Resize\n",
    "#         img_normalized = img_resized / 255.0  # Normalize pixel values to [0, 1]\n",
    "#         return img_normalized\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error preprocessing image: {e}\")\n",
    "#         return create_placeholder_image(size)\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# def preprocess_dataset(data):\n",
    "#     # Define placeholder image\n",
    "#     placeholder_image = create_placeholder_image()\n",
    "\n",
    "#     # Create missingness indicators\n",
    "#     data['Breakfast_Missing'] = data['Image Before Breakfast'].isnull().astype(int)\n",
    "#     data['Lunch_Missing'] = data['Image Before Lunch'].isnull().astype(int)\n",
    "\n",
    "#     # Iterate over rows to preprocess images\n",
    "#     breakfast_images = []\n",
    "#     lunch_images = []\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         # Handle missing breakfast images\n",
    "#         if pd.isnull(row['Image Before Breakfast']) or row['Image Before Breakfast'] == '[]':  # Check for empty list or NaN\n",
    "#             breakfast_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Breakfast'])  # Convert string to list\n",
    "#                 breakfast_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, breakfast: {e}\")\n",
    "#                 breakfast_images.append(placeholder_image)\n",
    "\n",
    "#         # Handle missing lunch images\n",
    "#         if pd.isnull(row['Image Before Lunch']) or row['Image Before Lunch'] == '[]':  # Check for empty list or NaN\n",
    "#             lunch_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Lunch'])  # Convert string to list\n",
    "#                 lunch_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, lunch: {e}\")\n",
    "#                 lunch_images.append(placeholder_image)\n",
    "\n",
    "#     # Add preprocessed images back to the dataset\n",
    "#     data['Processed_Breakfast_Images'] = breakfast_images\n",
    "#     data['Processed_Lunch_Images'] = lunch_images\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # Apply preprocessing\n",
    "# processed_data = preprocess_dataset(data)\n",
    "\n",
    "# # Save the processed dataset if needed\n",
    "# # processed_data.to_pickle(\"processed_img_train.pkl\")  # Save in pickle format for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# label_data = pd.read_csv(\"label_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Step 1: Extract Output Labels\n",
    "# output_labels = label_data[[\"Breakfast Calories\", \"Lunch Calories\"]]\n",
    "\n",
    "# # Step 2: Handle Missing Values in Labels\n",
    "# # Replace missing values (if any) with the median\n",
    "# output_labels = output_labels.fillna(output_labels.median())\n",
    "\n",
    "# print(output_labels)\n",
    "\n",
    "# # Step 3: Save the Extracted Labels\n",
    "# # output_labels.to_csv(\"output_labels.csv\", index=False)\n",
    "\n",
    "# print(\"Output Labels Extracted and Saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
