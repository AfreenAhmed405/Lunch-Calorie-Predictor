{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "# Training datasets\n",
    "cgm_train = pd.read_csv('cgm_train.csv')\n",
    "image_train = pd.read_csv('img_train.csv')\n",
    "demo_viome_train = pd.read_csv('demo_viome_train.csv')\n",
    "label_train = pd.read_csv('label_train.csv')\n",
    "\n",
    "# Test datasets\n",
    "cgm_test = pd.read_csv('cgm_test.csv')\n",
    "image_test = pd.read_csv('img_test.csv')\n",
    "demo_viome_test = pd.read_csv('demo_viome_test.csv')\n",
    "label_test = pd.read_csv('label_test_breakfast_only.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process CGM Data (Time-Series Glucose Levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if CGM Data is an empty array\n",
    "def is_cgm_data_empty(row):\n",
    "    try:\n",
    "        cgm_list = ast.literal_eval(row['CGM Data'])\n",
    "        return len(cgm_list) == 0\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# Apply the function to filter out rows where CGM Data is empty\n",
    "cgm_train = cgm_train[~cgm_train.apply(is_cgm_data_empty, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with empty CGM data have been removed. TODO: Put in function to reuse for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing breakfast and lunch times\n",
    "cgm_train['Breakfast Time'] = pd.to_datetime(cgm_train['Breakfast Time'], errors='coerce')\n",
    "cgm_train['Lunch Time'] = pd.to_datetime(cgm_train['Lunch Time'], errors='coerce')\n",
    "\n",
    "# Mask missing breakfast and lunch times for training\n",
    "cgm_train['Breakfast Time'] = cgm_train['Breakfast Time'].fillna('MASK')\n",
    "cgm_train['Lunch Time'] = cgm_train['Lunch Time'].fillna('MASK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing CGM data\n",
    "# Convert the CGM Data into a usable format (e.g., flatten the time-value pairs)\n",
    "cgm_train['CGM Data'] = cgm_train['CGM Data'].apply(eval).apply(lambda x: ' '.join([f'{t[0]}:{t[1]}' for t in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaad1ba646644b7916414d75e3ab491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maila\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maila\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d562c1ddcce467a9228e12d0bdb3bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2ce809c2b44ca193facef606426eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8774c702e842b397892863f7f4da69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30234230486457e856fe29a6b99e0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1886 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 2023-09-02 08:08:29 is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[241], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[241], line 14\u001b[0m, in \u001b[0;36mTimePredictionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcgm_data[idx]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbreakfast_times\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_lunch\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlunch_times[idx]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     16\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2751\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2771\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2772\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   2773\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2775\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[0;32m   2787\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2788\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2791\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3205\u001b[0m )\n\u001b[1;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:801\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m     )\n\u001b[1;32m--> 801\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    805\u001b[0m     first_ids,\n\u001b[0;32m    806\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    822\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:787\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `is_split_into_words=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m     )\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 2023-09-02 08:08:29 is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare for model training\n",
    "class TimePredictionDataset(Dataset):\n",
    "    def __init__(self, cgm_data, breakfast_times, lunch_times):\n",
    "        self.cgm_data = cgm_data\n",
    "        self.breakfast_times = breakfast_times\n",
    "        self.lunch_times = lunch_times\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cgm_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokenizer.encode(self.cgm_data[idx]), dtype=torch.long),\n",
    "            'labels': torch.tensor(tokenizer.encode(self.breakfast_times[idx]), dtype=torch.long),\n",
    "            'labels_lunch': torch.tensor(tokenizer.encode(self.lunch_times[idx]), dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "# Tokenizer and model setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare dataset and dataloaders\n",
    "train_dataset = TimePredictionDataset(cgm_train['CGM Data'], cgm_train['Breakfast Time'], cgm_train['Lunch Time'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Step 4: Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        labels_lunch = batch['labels_lunch']\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "model.save_pretrained('./time_prediction_model')\n",
    "tokenizer.save_pretrained('./time_prediction_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[224], line 2\u001b[0m\n    cgm_train['CGM Data'] = cgm_train['CGM Data'].apply(eval)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m in \u001b[0;35mapply\u001b[0m\n    ).apply()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m in \u001b[0;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m in \u001b[0;35mapply_standard\u001b[0m\n    mapped = obj._map_values(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m in \u001b[0;35m_map_values\u001b[0m\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\maila\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m in \u001b[0;35mmap_array\u001b[0m\n    return lib.map_infer(values, mapper, convert=convert)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mlib.pyx:2972\u001b[1;36m in \u001b[1;35mpandas._libs.lib.map_infer\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    2021-09-19 08:20:00:98.26666666666667 2021-09-19 08:25:00:95.18333333333334 2021-09-19 08:30:00:97.28333333333333 2021-09-19 08:35:00:106.11666666666666 2021-09-19 08:40:00:121.65 2021-09-19 08:45:00:131.38333333333333 2021-09-19 08:50:00:141.81666666666666 2021-09-19 08:55:00:138.25 2021-09-19 09:00:00:122.7 2021-09-19 09:05:00:106.71666666666667 2021-09-19 09:10:00:99.9 2021-09-19 09:15:00:94.81666666666666 2021-09-19 09:20:00:94.36666666666666 2021-09-19 09:25:00:96.55 2021-09-19 09:30:00:100.83333333333333 2021-09-19 09:35:00:109.55 2021-09-19 09:40:00:112.18333333333334 2021-09-19 09:45:00:113.91362126245848 2021-09-19 09:50:00:118.72240802675586 2021-09-19 09:55:00:122.73333333333333 2021-09-19 10:00:00:124.16666666666667 2021-09-19 10:05:00:110.86666666666667 2021-09-19 10:10:00:82.68333333333334 2021-09-19 10:15:00:56.43333333333334 2021-09-19 10:20:00:45.18333333333333 2021-09-19 10:25:00:46.55 2021-09-19 10:30:00:49.55 2021-09-19 10:35:00:56.21666666666667 2021-09-19 10:40:00:78.85 2021-09-19 10:45:00:97.83333333333333 2021-09-19 10:50:00:105.63333333333334 2021-09-19 10:55:00:102.53333333333333 2021-09-19 11:00:00:96.73333333333333 2021-09-19 11:05:00:100.0 2021-09-19 11:10:00:99.45 2021-09-19 11:15:00:94.8 2021-09-19 11:20:00:84.45 2021-09-19 11:25:00:82.0 2021-09-19 11:30:00:82.91666666666667 2021-09-19 11:35:00:87.18333333333334 2021-09-19 11:40:00:87.45 2021-09-19 11:45:00:84.08333333333333 2021-09-19 11:50:00:79.81666666666666 2021-09-19 11:55:00:79.91666666666667 2021-09-19 12:00:00:85.28333333333333 2021-09-19 12:05:00:92.1 2021-09-19 12:10:00:97.73333333333333 2021-09-19 12:15:00:102.28333333333333 2021-09-19 12:20:00:109.1 2021-09-19 12:25:00:114.91666666666667 2021-09-19 12:30:00:121.38333333333334 2021-09-19 12:35:00:132.55 2021-09-19 12:40:00:133.16666666666666 2021-09-19 12:45:00:121.51666666666667 2021-09-19 12:50:00:101.05 2021-09-19 12:55:00:75.15 2021-09-19 13:00:00:55.43333333333334 2021-09-19 13:05:00:43.266666666666666 2021-09-19 13:10:00:40.733333333333334 2021-09-19 13:15:00:43.63333333333333 2021-09-19 13:20:00:45.666666666666664 2021-09-19 13:25:00:65.3 2021-09-19 13:30:00:82.01666666666667 2021-09-19 13:35:00:90.45 2021-09-19 13:40:00:88.18333333333334 2021-09-19 13:45:00:89.73333333333333 2021-09-19 13:50:00:92.81666666666666 2021-09-19 13:55:00:91.45 2021-09-19 14:00:00:88.26666666666667 2021-09-19 14:05:00:84.26666666666667 2021-09-19 14:10:00:80.45 2021-09-19 14:15:00:77.81666666666666 2021-09-19 14:20:00:77.36666666666666 2021-09-19 14:25:00:79.55 2021-09-19 14:30:00:81.81666666666666 2021-09-19 14:35:00:81.18333333333334 2021-09-19 14:40:00:82.18333333333334 2021-09-19 14:45:00:83.55 2021-09-19 14:50:00:86.18333333333334 2021-09-19 14:55:00:86.81666666666666 2021-09-19 15:00:00:86.36666666666666 2021-09-19 15:05:00:88.55 2021-09-19 15:10:00:90.63333333333334 2021-09-19 15:15:00:88.81666666666666 2021-09-19 15:20:00:88.0 2021-09-19 15:25:00:87.81666666666666 2021-09-19 15:30:00:86.81666666666666 2021-09-19 15:35:00:86.0 2021-09-19 15:40:00:85.26666666666667 2021-09-19 15:45:00:80.71666666666667 2021-09-19 15:50:00:74.63333333333334 2021-09-19 15:55:00:77.58333333333333 2021-09-19 16:00:00:98.18333333333334\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "# Convert CGM Data from string to list of tuples\n",
    "cgm_train['CGM Data'] = cgm_train['CGM Data'].apply(eval)\n",
    "\n",
    "# Create a fixed time grid from 00:00 to 23:55 with 5-minute intervals\n",
    "full_time_range = pd.date_range('2021-09-19 00:00', '2021-09-19 23:55', freq='5min')\n",
    "\n",
    "def preprocess_subject_data(subject_data):\n",
    "        breakfast_time = pd.to_datetime(subject_data['Breakfast Time'], errors='coerce')\n",
    "        lunch_time = pd.to_datetime(subject_data['Lunch Time'], errors='coerce')\n",
    "\n",
    "        cgm_df = pd.DataFrame(subject_data['CGM Data'], columns=['timestamp', 'glucose_level'])\n",
    "        print(cgm_df)\n",
    "        # cgm_df['timestamp'] = pd.to_datetime(cgm_df['timestamp'], errors='coerce')\n",
    "        # cgm_df.dropna(subset=['timestamp'], inplace=True)\n",
    "        # cgm_df.set_index('timestamp', inplace=True)\n",
    "        # cgm_resampled = cgm_df.reindex(full_time_range, method=None)\n",
    "        # cgm_resampled['glucose_level'] = cgm_resampled['glucose_level'].interpolate(method='time')\n",
    "\n",
    "        # scaler = MinMaxScaler()\n",
    "        # cgm_resampled['scaled_glucose'] = scaler.fit_transform(cgm_resampled[['glucose_level']])\n",
    "\n",
    "        # fixed_length = 288\n",
    "        # padded_glucose = pad_sequences(cgm_resampled[['scaled_glucose']].values.T, maxlen=fixed_length, padding='post', value=0)\n",
    "\n",
    "        # return padded_glucose.flatten()\n",
    "\n",
    "preprocessed_cgms = np.array(cgm_train.apply(preprocess_subject_data, axis=1))\n",
    "# print(preprocessed_cgms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load dataset with proper delimiter (ensure '\\t' for tab-separated values)\n",
    "# file_path = 'demo_viome_train.csv'\n",
    "# data = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# # Recheck column parsing\n",
    "# if len(data.columns) == 1:\n",
    "#     # If all data is in a single column, try splitting with a comma\n",
    "#     data = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# # Verify column names\n",
    "# print(\"Columns in dataset after re-parsing:\", data.columns)\n",
    "\n",
    "# # Split the `Viome` column into individual features\n",
    "# viome_split = data['Viome'].str.split(',', expand=True).astype(float)\n",
    "# viome_split.columns = [f\"Viome_{i}\" for i in range(viome_split.shape[1])]\n",
    "\n",
    "# # Drop the original Viome column and merge new features\n",
    "# data = pd.concat([data.drop(columns=['Viome']), viome_split], axis=1)\n",
    "\n",
    "# # Impute missing values for numeric columns\n",
    "# numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "# # Normalize numeric data\n",
    "# scaler = MinMaxScaler()\n",
    "# data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "\n",
    "# # Encode categorical columns\n",
    "# categorical_cols = ['Gender', 'Race', 'Diabetes Status']\n",
    "# encoder = OneHotEncoder(sparse_output=False, drop='first')  # Use sparse_output instead of sparse\n",
    "# encoded_cats = pd.DataFrame(\n",
    "#     encoder.fit_transform(data[categorical_cols]),\n",
    "#     columns=encoder.get_feature_names_out(categorical_cols)\n",
    "# )\n",
    "\n",
    "# # Drop original categorical columns and merge encoded ones\n",
    "# data = pd.concat([data.drop(columns=categorical_cols), encoded_cats], axis=1)\n",
    "\n",
    "# # Final processed data\n",
    "# print(\"Processed Data Shape:\", data.shape)\n",
    "# print(\"Processed Data Preview:\")\n",
    "# print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv(\"img_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Placeholder for missing images (a blank black image)\n",
    "# def create_placeholder_image(size=(64, 64, 3)):\n",
    "#     return np.zeros(size, dtype=np.float32)  # Normalized [0, 1] range\n",
    "\n",
    "# # Function to preprocess image data\n",
    "# def preprocess_image(img_data, size=(64, 64)):\n",
    "#     try:\n",
    "#         img_array = np.array(img_data, dtype=np.uint8)  # Ensure valid data type\n",
    "\n",
    "#         # Check for empty image\n",
    "#         if img_array.size == 0 or img_array.ndim != 3 or img_array.shape[2] != 3:\n",
    "#             raise ValueError(f\"Invalid or empty image dimensions: {img_array.shape}\")\n",
    "\n",
    "#         img_resized = np.array(Image.fromarray(img_array).resize(size))  # Resize\n",
    "#         img_normalized = img_resized / 255.0  # Normalize pixel values to [0, 1]\n",
    "#         return img_normalized\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error preprocessing image: {e}\")\n",
    "#         return create_placeholder_image(size)\n",
    "\n",
    "# # Preprocess the dataset\n",
    "# def preprocess_dataset(data):\n",
    "#     # Define placeholder image\n",
    "#     placeholder_image = create_placeholder_image()\n",
    "\n",
    "#     # Create missingness indicators\n",
    "#     data['Breakfast_Missing'] = data['Image Before Breakfast'].isnull().astype(int)\n",
    "#     data['Lunch_Missing'] = data['Image Before Lunch'].isnull().astype(int)\n",
    "\n",
    "#     # Iterate over rows to preprocess images\n",
    "#     breakfast_images = []\n",
    "#     lunch_images = []\n",
    "\n",
    "#     for index, row in data.iterrows():\n",
    "#         # Handle missing breakfast images\n",
    "#         if pd.isnull(row['Image Before Breakfast']) or row['Image Before Breakfast'] == '[]':  # Check for empty list or NaN\n",
    "#             breakfast_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Breakfast'])  # Convert string to list\n",
    "#                 breakfast_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, breakfast: {e}\")\n",
    "#                 breakfast_images.append(placeholder_image)\n",
    "\n",
    "#         # Handle missing lunch images\n",
    "#         if pd.isnull(row['Image Before Lunch']) or row['Image Before Lunch'] == '[]':  # Check for empty list or NaN\n",
    "#             lunch_images.append(placeholder_image)\n",
    "#         else:\n",
    "#             try:\n",
    "#                 img_data = eval(row['Image Before Lunch'])  # Convert string to list\n",
    "#                 lunch_images.append(preprocess_image(img_data))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error at index {index}, lunch: {e}\")\n",
    "#                 lunch_images.append(placeholder_image)\n",
    "\n",
    "#     # Add preprocessed images back to the dataset\n",
    "#     data['Processed_Breakfast_Images'] = breakfast_images\n",
    "#     data['Processed_Lunch_Images'] = lunch_images\n",
    "\n",
    "#     return data\n",
    "\n",
    "# # Apply preprocessing\n",
    "# processed_data = preprocess_dataset(data)\n",
    "\n",
    "# # Save the processed dataset if needed\n",
    "# # processed_data.to_pickle(\"processed_img_train.pkl\")  # Save in pickle format for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# label_data = pd.read_csv(\"label_train.csv\")  # Adjust the file path as necessary\n",
    "\n",
    "# # Step 1: Extract Output Labels\n",
    "# output_labels = label_data[[\"Breakfast Calories\", \"Lunch Calories\"]]\n",
    "\n",
    "# # Step 2: Handle Missing Values in Labels\n",
    "# # Replace missing values (if any) with the median\n",
    "# output_labels = output_labels.fillna(output_labels.median())\n",
    "\n",
    "# print(output_labels)\n",
    "\n",
    "# # Step 3: Save the Extracted Labels\n",
    "# # output_labels.to_csv(\"output_labels.csv\", index=False)\n",
    "\n",
    "# print(\"Output Labels Extracted and Saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
